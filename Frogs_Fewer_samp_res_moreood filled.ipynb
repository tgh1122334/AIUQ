{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce results using same data but pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from lib_FrogsCNN import *\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "split_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(torch.utils.data.TensorDataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "def dataloader_Frogs(batch_size_train = 16, batch_size_test = 2, random_seed=112892):\n",
    "    torch.manual_seed(random_seed)\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\"./filled_images/\")\n",
    "    transform_image = torchvision.transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize([512, 512]),\n",
    "        # transforms.RandomRotation(degrees=90),\n",
    "        # transforms.RandomVerticalFlip(p=0.5),\n",
    "        # transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    vec_id = [\"Clade4\",\"Clade5\",\"Clade8\",\"Clade12\"]\n",
    "    vec_id_value = []\n",
    "    for ii in vec_id:\n",
    "        vec_id_value.append(train_dataset.class_to_idx[ii])\n",
    "    vec_ood_value = set(train_dataset.class_to_idx.values()).difference(vec_id_value)\n",
    "    dic_total = {train_dataset.class_to_idx[vec_id[i]]:i for i in range(4)}\n",
    "    s = pd.Series(data=dic_total.values(), index=dic_total.keys())\n",
    "\n",
    "    mat_dataset = np.empty(shape=(0,3,512,512))\n",
    "    vec_labels=np.empty(shape=0)\n",
    "    for _,data in enumerate(train_dataset):\n",
    "        mat_dataset = np.vstack((mat_dataset,transform_image(data[0]).reshape(1,3,512,512)))\n",
    "        vec_labels = np.append(vec_labels,data[1])\n",
    "    vec_ood_idx = []\n",
    "    vec_id_idx = []\n",
    "    for ii in range(len(vec_labels)):\n",
    "        if vec_labels[ii] in vec_ood_value:\n",
    "            vec_ood_idx.append(ii)\n",
    "        else:\n",
    "            vec_id_idx.append(ii)\n",
    "    mat_dataset_id = mat_dataset[vec_id_idx,:]\n",
    "    mat_dataset_ood = mat_dataset[vec_ood_idx,:]\n",
    "    vec_labels_id = vec_labels[vec_id_idx]\n",
    "    vec_labels_id_coded = np.array(s[vec_labels_id])\n",
    "    vec_labels_ood = vec_labels[vec_ood_idx]\n",
    "\n",
    "    X_train_id, X_test_id, y_train_id, y_test_id = train_test_split(mat_dataset_id, vec_labels_id_coded,\n",
    "                                   random_state=split_seed, \n",
    "                                   test_size=0.10, \n",
    "                                   shuffle=True)\n",
    "\n",
    "    X_train_ood, X_test_ood, y_train_ood, y_test_ood = train_test_split(mat_dataset_ood, vec_labels_ood,\n",
    "                                    random_state=split_seed, \n",
    "                                    test_size=0.10, \n",
    "                                    shuffle=True)\n",
    "\n",
    "    transform_image_v2 = torchvision.transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize([512, 512]),\n",
    "        # transforms.RandomRotation(degrees=90),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    train_dataset_id = CustomTensorDataset((torch.Tensor(X_train_id), torch.Tensor(y_train_id))\n",
    "        , transform=transform_image_v2)\n",
    "    train_dataset_ood = CustomTensorDataset((torch.Tensor(X_train_ood), torch.Tensor(y_train_ood))\n",
    "        , transform=transform_image_v2)\n",
    "\n",
    "    test_dataset_id = CustomTensorDataset((torch.Tensor(X_test_id), torch.Tensor(y_test_id))\n",
    "        , transform=transform_image_v2)\n",
    "    test_dataset_ood = CustomTensorDataset((torch.Tensor(X_test_ood), torch.Tensor(y_test_ood))\n",
    "        , transform=transform_image_v2)\n",
    "\n",
    "\n",
    "    train_loader_id = torch.utils.data.DataLoader(train_dataset_id,\n",
    "        batch_size=batch_size_train, shuffle=True)\n",
    "    train_loader_ood = torch.utils.data.DataLoader(train_dataset_ood,\n",
    "        batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "    test_loader_id = torch.utils.data.DataLoader(test_dataset_id,\n",
    "        batch_size=batch_size_test, shuffle=True)\n",
    "    test_loader_ood = torch.utils.data.DataLoader(test_dataset_ood,\n",
    "        batch_size=batch_size_test, shuffle=True)\n",
    "    \n",
    "    print(\"train_loader_id, train_loader_ood\")\n",
    "    return train_loader_id, train_loader_ood, test_loader_id, test_loader_ood,train_dataset.class_to_idx,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_id, train_loader_ood, test_loader_id, test_loader_ood, class_to_idx, df_coding = dataloader_Frogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_id.dataset.tensors[0].shape,test_loader_id.dataset.tensors[0].shape,\\\n",
    "train_loader_ood.dataset.tensors[0].shape,test_loader_ood.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "## define model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3)\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 4, kernel_size=3)\n",
    "        self.conv3_bn = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.fc1 = nn.Linear(15376, 256)\n",
    "        self.fc1_bn = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc2_bn = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc3_bn = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size = 2, stride = 2)\n",
    "        x = self.conv1_bn(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size = 2, stride = 2)\n",
    "        x = self.conv2_bn(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size = 2, stride = 2)\n",
    "        x = self.conv3_bn(x)\n",
    "\n",
    "\n",
    "        x = x.view(-1, 15376)\n",
    "        x = (self.fc1(x))\n",
    "        x = F.dropout(x, p= 0.8,training=self.training)\n",
    "        x = self.fc1_bn(x)\n",
    "\n",
    "        x = (self.fc2(x))\n",
    "        x = F.dropout(x, p= 0.8,training=self.training)\n",
    "        x = self.fc2_bn(x)\n",
    "\n",
    "        x = (self.fc3(x))\n",
    "        x = F.dropout(x, p= 0.8,training=self.training)\n",
    "        x = self.fc3_bn(x)\n",
    "\n",
    "        x = (self.fc4(x))\n",
    "        return x\n",
    "    \n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader_id):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(data.shape[0],-1,512,512)\n",
    "        data = data.cuda()\n",
    "        # target = target.argmax(axis = 1).view(-1,1)\n",
    "        target = target.cuda()\n",
    "        output = network(data)\n",
    "        loss = F.cross_entropy((output), target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader_id.dataset),\n",
    "                100. * batch_idx / len(train_loader_id), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*16) + ((epoch-1)*len(train_loader_id.dataset)))\n",
    "        data.cpu()\n",
    "        target.cpu()\n",
    "\n",
    "network = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "log_interval = 10\n",
    "n_epochs = 2000\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)#,\n",
    "                    #   momentum=momentum)\n",
    "network.cuda()\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader_id.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "network.cuda()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the module\n",
    "torch.save(network.state_dict(), \"seed_filled\"+str(split_seed)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n_epochs = 500\n",
    "# learning_rate = 0.001\n",
    "# network.cuda()\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the model\n",
    "# network.load_state_dict(torch.load(\"seed_111.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_label = np.empty(0)\n",
    "true_label = np.empty(0)\n",
    "network.cpu()\n",
    "network.eval()\n",
    "for batch_idx, (data, target) in enumerate(train_loader_id):\n",
    "    true_label = np.append(true_label, target)\n",
    "    out = network(data)\n",
    "    pre_label = np.append(pre_label, np.exp(out.cpu().detach().numpy()).argmax(axis = 1))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(true_label, pre_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_label = np.empty(0)\n",
    "true_label = np.empty(0)\n",
    "network.cpu()\n",
    "network.eval()\n",
    "for batch_idx, (data, target) in enumerate(test_loader_id):\n",
    "    true_label = np.append(true_label, target)\n",
    "    out = network(data)\n",
    "    pre_label = np.append(pre_label, np.exp(out.cpu().detach().numpy()).argmax(axis = 1))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(true_label, pre_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get intermediate output\n",
    "class Hook():\n",
    "    def __init__(self, module, backward=False):\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "hookF = [Hook(layer[1]) for layer in list(network._modules.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, (data, target) = next(enumerate(train_loader_id))\n",
    "out=network(data)\n",
    "\n",
    "print('***'*3+'  Forward Hooks Inputs & Outputs  '+'***'*3)\n",
    "ll=0\n",
    "for hook in hookF[:6]:\n",
    "    print(ll)\n",
    "    ll=ll+1\n",
    "    print(hook.input[0].shape)\n",
    "    print(hook.output.shape)\n",
    "    print('---'*17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii = 1\n",
    "# from matplotlib.transforms import Bbox\n",
    "# import cv2\n",
    "# my_dpi = 100 # Good default - doesn't really matter\n",
    "# fig, ax = plt.subplots(1, figsize=(8,8))\n",
    "# ax.set_position([0, 0, 1, 1]) # Critical!\n",
    "# plt.imshow(np.moveaxis(np.array(hookF[0].input[0][ii,:,:,:]),0,2))\n",
    "# ax.axis(\"off\")\n",
    "# # fig.savefig(\"LayerInput_\"+str(ii)+\".pdf\",bbox_inches='tight')\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(4,4, figsize=(8,8))\n",
    "# ll=0\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         axs[i][j].imshow(hookF[4].input[0][ii,ll,:].detach().numpy(), \"gray\")\n",
    "#         axs[i][j].axis(\"off\")\n",
    "#         axs[i][j].set_xticklabels([])\n",
    "#         axs[i][j].set_yticklabels([])\n",
    "#         axs[i][j].set_aspect('equal')\n",
    "#         axs[i][j].set_position([0, 0, 1, 1])\n",
    "#         ll=ll+1\n",
    "# plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.01, hspace=0.01)\n",
    "# # plt.savefig(\"LayerConv3_\"+str(ii)+\".pdf\",bbox_inches='tight')\n",
    "\n",
    "# fig, axs = plt.subplots(1,4, figsize=[32,16])\n",
    "# for i in range(4):\n",
    "#     axs[i].imshow(hookF[5].input[0][ii,i,:].detach().numpy(),\"gray\")\n",
    "#     axs[i].axis(\"off\")\n",
    "#     axs[i].set_xticklabels([])\n",
    "#     axs[i].set_yticklabels([])\n",
    "#     axs[i].set_aspect('equal')\n",
    "# plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.01, hspace=0)\n",
    "# # plt.savefig(\"LayerConv4-a_\"+str(ii)+\".pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (data, target) in (enumerate(train_loader_id)):\n",
    "#     out=network(data)\n",
    "#     print(hookF[0].input[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.transforms import Bbox\n",
    "# import cv2\n",
    "# pp = PdfPages(\"temp.pdf\")\n",
    "# for batch_idx, (data, target) in (enumerate(train_loader_id)):\n",
    "#     out=network(data)\n",
    "#     for ii in range(hookF[0].input[0].shape[0]):\n",
    "#         fig, ax = plt.subplots(1, figsize=(8,8))\n",
    "#         ax.set_position([0, 0, 1, 1]) # Critical!\n",
    "#         plt.imshow(np.moveaxis(np.array(hookF[0].input[0][ii,:,:,:]),0,2))\n",
    "#         ax.axis(\"off\")\n",
    "#         pp.savefig(fig)\n",
    "\n",
    "#         fig, axs = plt.subplots(4,4, figsize=(8,8))\n",
    "#         ll=0\n",
    "#         for i in range(4):\n",
    "#             for j in range(4):\n",
    "#                 axs[i][j].imshow(hookF[4].input[0][ii,ll,:].detach().numpy(), \"gray\")\n",
    "#                 axs[i][j].axis(\"off\")\n",
    "#                 axs[i][j].set_xticklabels([])\n",
    "#                 axs[i][j].set_yticklabels([])\n",
    "#                 axs[i][j].set_aspect('equal')\n",
    "#                 axs[i][j].set_position([0, 0, 1, 1])\n",
    "#                 ll=ll+1\n",
    "#         plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.01, hspace=0.01)\n",
    "#         pp.savefig(fig)\n",
    "\n",
    "#         fig, axs = plt.subplots(1,4, figsize=[32,16])\n",
    "#         for i in range(4):\n",
    "#             axs[i].imshow(hookF[5].input[0][ii,i,:].detach().numpy(),\"gray\")\n",
    "#             axs[i].axis(\"off\")\n",
    "#             axs[i].set_xticklabels([])\n",
    "#             axs[i].set_yticklabels([])\n",
    "#             axs[i].set_aspect('equal')\n",
    "#         plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.01, hspace=0)\n",
    "#         pp.savefig(fig)\n",
    "# pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_lay128=np.array([]).reshape(0,128)\n",
    "train_id_lay64=np.array([]).reshape(0,64)\n",
    "train_id_lay4=np.array([]).reshape(0,4)\n",
    "true_target = np.array([]).reshape(-1)\n",
    "for batch_idx, (data, target) in enumerate(train_loader_id):\n",
    "    out=network(data)\n",
    "    train_id_lay128 = np.vstack([train_id_lay128,hookF[8].output.detach().numpy()])\n",
    "    train_id_lay64 = np.vstack([train_id_lay64,hookF[10].output.detach().numpy()])\n",
    "    train_id_lay4 = np.vstack([train_id_lay4,hookF[12].output.detach().numpy()])\n",
    "    true_target = np.concatenate((true_target, target))\n",
    "\n",
    "df_train_id_128 = pd.DataFrame(train_id_lay128, columns=[\"lay128_\"+str(i) for i in range(128)])\n",
    "df_train_id_128['dsource'] = \"train_id\"\n",
    "df_train_id_128['true_lab']=true_target\n",
    "\n",
    "df_train_id_64 = pd.DataFrame(train_id_lay64, columns=[\"lay64_\"+str(i) for i in range(64)])\n",
    "df_train_id_64['dsource'] = \"train_id\"\n",
    "df_train_id_64['true_lab']=true_target\n",
    "\n",
    "df_train_id_4 = pd.DataFrame(train_id_lay4, columns=[\"lay4_\"+str(i) for i in range(4)])\n",
    "df_train_id_4['dsource'] = \"train_id\"\n",
    "df_train_id_4['true_lab']=true_target\n",
    "\n",
    "\n",
    "train_ood_lay128=np.array([]).reshape(0,128)\n",
    "train_ood_lay64=np.array([]).reshape(0,64)\n",
    "train_ood_lay4=np.array([]).reshape(0,4)\n",
    "for batch_idx, (data, target) in enumerate(train_loader_ood):\n",
    "    out=network(data)\n",
    "    train_ood_lay128 = np.vstack([train_ood_lay128,hookF[8].output.detach().numpy()])\n",
    "    train_ood_lay64 = np.vstack([train_ood_lay64,hookF[10].output.detach().numpy()])\n",
    "    train_ood_lay4 = np.vstack([train_ood_lay4,hookF[12].output.detach().numpy()])\n",
    "\n",
    "\n",
    "df_train_ood_128 = pd.DataFrame(train_ood_lay128, columns=[\"lay128_\"+str(i) for i in range(128)])\n",
    "df_train_ood_128['dsource'] = \"train_ood\"\n",
    "df_train_ood_128['true_lab']=-1\n",
    "\n",
    "df_train_ood_64 = pd.DataFrame(train_ood_lay64, columns=[\"lay64_\"+str(i) for i in range(64)])\n",
    "df_train_ood_64['dsource'] = \"train_ood\"\n",
    "df_train_ood_64['true_lab']=-1\n",
    "\n",
    "df_train_ood_4 = pd.DataFrame(train_ood_lay4, columns=[\"lay4_\"+str(i) for i in range(4)])\n",
    "df_train_ood_4['dsource'] = \"train_ood\"\n",
    "df_train_ood_4['true_lab']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_128_all = pd.concat((df_train_id_128, df_train_ood_128))\n",
    "df_train_64_all = pd.concat((df_train_id_64, df_train_ood_64))\n",
    "df_train_4_all = pd.concat((df_train_id_4, df_train_ood_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id_lay128=np.array([]).reshape(0,128)\n",
    "test_id_lay64=np.array([]).reshape(0,64)\n",
    "test_id_lay4=np.array([]).reshape(0,4)\n",
    "true_target = np.array([]).reshape(-1)\n",
    "for batch_idx, (data, target) in enumerate(test_loader_id):\n",
    "    out=network(data)\n",
    "    test_id_lay128 = np.vstack([test_id_lay128,hookF[8].output.detach().numpy()])\n",
    "    test_id_lay64 = np.vstack([test_id_lay64,hookF[10].output.detach().numpy()])\n",
    "    test_id_lay4 = np.vstack([test_id_lay4,hookF[12].output.detach().numpy()])\n",
    "    true_target = np.concatenate((true_target, target))\n",
    "\n",
    "df_test_id_128 = pd.DataFrame(test_id_lay128, columns=[\"lay128_\"+str(i) for i in range(128)])\n",
    "df_test_id_128['dsource'] = \"test_id\"\n",
    "df_test_id_128['true_lab']=true_target\n",
    "\n",
    "df_test_id_64 = pd.DataFrame(test_id_lay64, columns=[\"lay64_\"+str(i) for i in range(64)])\n",
    "df_test_id_64['dsource'] = \"test_id\"\n",
    "df_test_id_64['true_lab']=true_target\n",
    "\n",
    "df_test_id_4 = pd.DataFrame(test_id_lay4, columns=[\"lay4_\"+str(i) for i in range(4)])\n",
    "df_test_id_4['dsource'] = \"test_id\"\n",
    "df_test_id_4['true_lab']=true_target\n",
    "\n",
    "\n",
    "test_ood_lay128=np.array([]).reshape(0,128)\n",
    "test_ood_lay64=np.array([]).reshape(0,64)\n",
    "test_ood_lay4=np.array([]).reshape(0,4)\n",
    "for batch_idx, (data, target) in enumerate(test_loader_ood):\n",
    "    out=network(data)\n",
    "    test_ood_lay128 = np.vstack([test_ood_lay128,hookF[8].output.detach().numpy()])\n",
    "    test_ood_lay64 = np.vstack([test_ood_lay64,hookF[10].output.detach().numpy()])\n",
    "    test_ood_lay4 = np.vstack([test_ood_lay4,hookF[12].output.detach().numpy()])\n",
    "\n",
    "\n",
    "df_test_ood_128 = pd.DataFrame(test_ood_lay128, columns=[\"lay128_\"+str(i) for i in range(128)])\n",
    "df_test_ood_128['dsource'] = \"test_ood\"\n",
    "df_test_ood_128['true_lab']=-1\n",
    "\n",
    "df_test_ood_64 = pd.DataFrame(test_ood_lay64, columns=[\"lay64_\"+str(i) for i in range(64)])\n",
    "df_test_ood_64['dsource'] = \"test_ood\"\n",
    "df_test_ood_64['true_lab']=-1\n",
    "\n",
    "df_test_ood_4 = pd.DataFrame(test_ood_lay4, columns=[\"lay4_\"+str(i) for i in range(4)])\n",
    "df_test_ood_4['dsource'] = \"test_ood\"\n",
    "df_test_ood_4['true_lab']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_128_all = pd.concat((df_test_id_128, df_test_ood_128))\n",
    "df_test_64_all = pd.concat((df_test_id_64, df_test_ood_64))\n",
    "df_test_4_all = pd.concat((df_test_id_4, df_test_ood_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##read the saved results\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "exec(open(\"./lib_RMD_v2.py\").read())\n",
    "\n",
    "# df_128_all = pd.read_csv(\"saved_res/layer_128_ferwer.csv\", index_col=0)\n",
    "# df_64_all= pd.read_csv(\"saved_res/layer_64_ferwer.csv\", index_col=0)\n",
    "# df_3_all= pd.read_csv(\"saved_res/layer_3_ferwer.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "exec(open(\"./lib_RMD_v2.py\").read())\n",
    "df_train_QDA_robust, df_test_QDA_robust = df_mahdist_QDA(\n",
    "    (df_train_128_all, df_train_64_all, df_train_4_all),\n",
    "    (df_test_128_all, df_test_64_all, df_test_4_all),\n",
    "    [128, 64, 4],4, Shrinkage = True)\n",
    "df_train_LDA_robust, df_test_LDA_robust = df_mahdist_LDA(\n",
    "    (df_train_128_all, df_train_64_all, df_train_4_all),\n",
    "    (df_test_128_all, df_test_64_all, df_test_4_all),\n",
    "    [128, 64, 4],4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "plt.hist(df_train_QDA_robust[df_train_QDA_robust['dsource']==\"train_id\"].dist_l128, alpha=.4, label='train_id')\n",
    "plt.hist(df_train_QDA_robust[df_train_QDA_robust['dsource']==\"train_ood\"].dist_l128, alpha=.4, label='train_ood')\n",
    "plt.hist(df_test_QDA_robust[df_test_QDA_robust['dsource']==\"test_id\"].dist_l128, alpha=.4, label='test_id')\n",
    "plt.hist(df_test_QDA_robust[df_test_QDA_robust['dsource']==\"test_ood\"].dist_l128, alpha=.4, label='test_ood')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Mahalanobis Distance\")\n",
    "plt.ylabel(\"Count\")\n",
    "# plt.title(\"Distribution of Mahalanobis Distance for 128-unit Layer\")\n",
    "# plt.savefig('Frog_layer128_'+str(ii)+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "plt.hist(df_train_QDA_robust[df_train_QDA_robust['dsource']==\"train_id\"].dist_l64, alpha=.4, label='train_id')\n",
    "plt.hist(df_train_QDA_robust[df_train_QDA_robust['dsource']==\"train_ood\"].dist_l64, alpha=.4, label='train_ood')\n",
    "plt.hist(df_test_QDA_robust[df_test_QDA_robust['dsource']==\"test_id\"].dist_l64, alpha=.4, label='test_id')\n",
    "plt.hist(df_test_QDA_robust[df_test_QDA_robust['dsource']==\"test_ood\"].dist_l64, alpha=.4, label='test_ood')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Mahalanobis Distance\")\n",
    "plt.ylabel(\"Count\")\n",
    "# plt.savefig('Frog_layer64_'+str(ii)+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "plt.hist(df_train_QDA_robust[df_train_QDA_robust['dsource']==\"train_id\"].dist_l4, alpha=.4, label='train_id')\n",
    "plt.hist(df_train_QDA_robust[df_train_QDA_robust['dsource']==\"train_ood\"].dist_l4, alpha=.4, label='train_ood')\n",
    "plt.hist(df_test_QDA_robust[df_test_QDA_robust['dsource']==\"test_id\"].dist_l4, alpha=.4, label='test_id')\n",
    "plt.hist(df_test_QDA_robust[df_test_QDA_robust['dsource']==\"test_ood\"].dist_l4, alpha=.4, label='test_ood')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Mahalanobis Distance\")\n",
    "plt.ylabel(\"Count\")\n",
    "# plt.savefig('Frog_layer3_'+str(ii)+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "df_train_QDA_robust['Y']=pd.Series([0 for x in range(len(df_train_QDA_robust.index))])\n",
    "df_train_QDA_robust.loc[df_train_QDA_robust['dsource']==\"train_ood\",'Y'] = pd.Series([1 for x in range(np.sum(df_train_QDA_robust['dsource']==\"train_ood\"))])\n",
    "df_test_QDA_robust['Y']=pd.Series([0 for x in range(len(df_test_QDA_robust.index))])\n",
    "df_test_QDA_robust.loc[df_test_QDA_robust['dsource']==\"test_ood\",'Y'] = pd.Series([1 for x in range(np.sum(df_test_QDA_robust['dsource']==\"test_ood\"))])\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(\n",
    "    df_train_QDA_robust.loc[:,['dist_l4','dist_l64','dist_l128']],\n",
    "    df_train_QDA_robust.loc[:,\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred_proba = clf.predict_proba(df_train_QDA_robust.loc[:,['dist_l4','dist_l64','dist_l128']])[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(df_train_QDA_robust.loc[:,'Y'],  y_pred_proba)\n",
    "\n",
    "auc = metrics.roc_auc_score(df_train_QDA_robust.loc[:,'Y'], y_pred_proba)\n",
    "print(auc)\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.savefig(\"QDA_ROC_filled.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(df_test_QDA_robust.loc[:,['dist_l4','dist_l64','dist_l128']])[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(df_test_QDA_robust.loc[:,'Y'],  y_pred_proba)\n",
    "\n",
    "auc = metrics.roc_auc_score(df_test_QDA_robust.loc[:,'Y'], y_pred_proba)\n",
    "print(auc)\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.savefig(\"QDA_ROC_prediction_filled.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "vec_pre = clf.predict(df_train_QDA_robust.loc[df_train_QDA_robust['dsource'].isin([\"train_id\",\"train_ood\"]),['dist_l4','dist_l64','dist_l128']])\n",
    "confusion_matrix(df_train_QDA_robust.loc[df_train_QDA_robust['dsource'].isin([\"train_id\",\"train_ood\"]),'Y'], vec_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "vec_pre = clf.predict(df_test_QDA_robust.loc[df_test_QDA_robust['dsource'].isin([\"test_id\",\"test_ood\"]),['dist_l4','dist_l64','dist_l128']])\n",
    "confusion_matrix(df_test_QDA_robust.loc[df_test_QDA_robust['dsource'].isin([\"test_id\",\"test_ood\"]),'Y'], vec_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "df_train_LDA_robust['Y']=pd.Series([0 for x in range(len(df_train_LDA_robust.index))])\n",
    "df_train_LDA_robust.loc[df_train_LDA_robust['dsource']==\"train_ood\",'Y'] = pd.Series([1 for x in range(np.sum(df_train_LDA_robust['dsource']==\"train_ood\"))])\n",
    "df_test_LDA_robust['Y']=pd.Series([0 for x in range(len(df_test_LDA_robust.index))])\n",
    "df_test_LDA_robust.loc[df_test_LDA_robust['dsource']==\"test_ood\",'Y'] = pd.Series([1 for x in range(np.sum(df_test_LDA_robust['dsource']==\"test_ood\"))])\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(\n",
    "    df_train_LDA_robust.loc[:,['dist_l4','dist_l64','dist_l128']],\n",
    "    df_train_LDA_robust.loc[:,\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred_proba = clf.predict_proba(df_train_LDA_robust.loc[:,['dist_l4','dist_l64','dist_l128']])[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(df_train_LDA_robust.loc[:,'Y'],  y_pred_proba)\n",
    "\n",
    "auc = metrics.roc_auc_score(df_train_LDA_robust.loc[:,'Y'], y_pred_proba)\n",
    "print(auc)\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.savefig(\"LDA_ROC_filled.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(df_test_LDA_robust.loc[:,['dist_l4','dist_l64','dist_l128']])[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(df_test_LDA_robust.loc[:,'Y'],  y_pred_proba)\n",
    "\n",
    "auc = metrics.roc_auc_score(df_test_LDA_robust.loc[:,'Y'], y_pred_proba)\n",
    "print(auc)\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.savefig(\"LDA_ROC_prediction_filled.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "vec_pre = clf.predict(df_train_LDA_robust.loc[df_train_LDA_robust['dsource'].isin([\"train_id\",\"train_ood\"]),['dist_l4','dist_l64','dist_l128']])\n",
    "confusion_matrix(df_train_LDA_robust.loc[df_train_LDA_robust['dsource'].isin([\"train_id\",\"train_ood\"]),'Y'], vec_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "vec_pre = clf.predict(df_test_LDA_robust.loc[df_test_LDA_robust['dsource'].isin([\"test_id\",\"test_ood\"]),['dist_l4','dist_l64','dist_l128']])\n",
    "confusion_matrix(df_test_LDA_robust.loc[df_test_LDA_robust['dsource'].isin([\"test_id\",\"test_ood\"]),'Y'], vec_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_QDA_robust = pd.concat([df_train_QDA_robust,df_test_QDA_robust])\n",
    "df_QDA_robust = df_QDA_robust.rename({\"dsource\":\"data_type\"}, axis=\"columns\")\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "p = sns.kdeplot(data=df_QDA_robust, x='dist_l128', hue='data_type',\n",
    "    cumulative=True, common_norm=False, common_grid=True, alpha=0.7,\n",
    "    linewidth = 2.5)\n",
    "lss = [':', '--', '-.', '-']\n",
    "handles = p.legend_.legendHandles[::-1]\n",
    "for line, ls, handle in zip(p.lines, lss, handles):\n",
    "    line.set_linestyle(ls)\n",
    "    handle.set_ls(ls)\n",
    "p.set(xlabel='Mahalanobis Distance', ylabel='Probability')\n",
    "plt.savefig(\"QDA_fill_cdf_128.pdf\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "p = sns.kdeplot(data=df_QDA_robust, x='dist_l64', hue='data_type',\n",
    "    cumulative=True, common_norm=False, common_grid=True, alpha=0.7,\n",
    "    linewidth = 2.5)\n",
    "lss = [':', '--', '-.', '-']\n",
    "handles = p.legend_.legendHandles[::-1]\n",
    "for line, ls, handle in zip(p.lines, lss, handles):\n",
    "    line.set_linestyle(ls)\n",
    "    handle.set_ls(ls)\n",
    "p.set(xlabel='Mahalanobis Distance', ylabel='Probability')\n",
    "plt.savefig(\"QDA_fill_cdf_64.pdf\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "p = sns.kdeplot(data=df_QDA_robust, x='dist_l4', hue='data_type',\n",
    "    cumulative=True, common_norm=False, common_grid=True, alpha=0.7,\n",
    "    linewidth = 2.5)\n",
    "lss = [':', '--', '-.', '-']\n",
    "handles = p.legend_.legendHandles[::-1]\n",
    "for line, ls, handle in zip(p.lines, lss, handles):\n",
    "    line.set_linestyle(ls)\n",
    "    handle.set_ls(ls)\n",
    "p.set(xlabel='Mahalanobis Distance', ylabel='Probability')\n",
    "plt.savefig(\"QDA_fill_cdf_8.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_LDA_robust = pd.concat([df_train_LDA_robust,df_test_LDA_robust])\n",
    "df_LDA_robust = df_LDA_robust.rename({\"dsource\":\"data_type\"}, axis=\"columns\")\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "p = sns.kdeplot(data=df_LDA_robust, x='dist_l128', hue='data_type',\n",
    "    cumulative=True, common_norm=False, common_grid=True, alpha=0.7,\n",
    "    linewidth = 2.5)\n",
    "lss = [':', '--', '-.', '-']\n",
    "handles = p.legend_.legendHandles[::-1]\n",
    "for line, ls, handle in zip(p.lines, lss, handles):\n",
    "    line.set_linestyle(ls)\n",
    "    handle.set_ls(ls)\n",
    "p.set(xlabel='Mahalanobis Distance', ylabel='Probability')\n",
    "plt.savefig(\"LDA_fill_cdf_128.pdf\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "p = sns.kdeplot(data=df_LDA_robust, x='dist_l64', hue='data_type',\n",
    "    cumulative=True, common_norm=False, common_grid=True, alpha=0.7,\n",
    "    linewidth = 2.5)\n",
    "lss = [':', '--', '-.', '-']\n",
    "handles = p.legend_.legendHandles[::-1]\n",
    "for line, ls, handle in zip(p.lines, lss, handles):\n",
    "    line.set_linestyle(ls)\n",
    "    handle.set_ls(ls)\n",
    "p.set(xlabel='Mahalanobis Distance', ylabel='Probability')\n",
    "plt.savefig(\"LDA_fill_cdf_64.pdf\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "p = sns.kdeplot(data=df_LDA_robust, x='dist_l4', hue='data_type',\n",
    "    cumulative=True, common_norm=False, common_grid=True, alpha=0.7,\n",
    "    linewidth = 2.5)\n",
    "lss = [':', '--', '-.', '-']\n",
    "handles = p.legend_.legendHandles[::-1]\n",
    "for line, ls, handle in zip(p.lines, lss, handles):\n",
    "    line.set_linestyle(ls)\n",
    "    handle.set_ls(ls)\n",
    "p.set(xlabel='Mahalanobis Distance', ylabel='Probability')\n",
    "plt.savefig(\"LDA_fill_cdf_8.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "9081dc37459498f3d0807db8daad12194205b9edd8a0b3e7b10105e9e26d7ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
